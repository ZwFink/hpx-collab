Thu Jan 13 18:17:29 PST 2022
Architecture:        x86_64
CPU op-mode(s):      32-bit, 64-bit
Byte Order:          Little Endian
CPU(s):              64
On-line CPU(s) list: 0-63
Thread(s) per core:  2
Core(s) per socket:  16
Socket(s):           2
NUMA node(s):        2
Vendor ID:           GenuineIntel
CPU family:          6
Model:               63
Model name:          Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz
Stepping:            2
CPU MHz:             2301.000
CPU max MHz:         2301.0000
CPU min MHz:         1200.0000
BogoMIPS:            4600.15
Virtualization:      VT-x
L1d cache:           32K
L1i cache:           32K
L2 cache:            256K
L3 cache:            40960K
NUMA node0 CPU(s):   0-15,32-47
NUMA node1 CPU(s):   16-31,48-63
Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm cpuid_fault epb invpcid_single pti intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear flush_l1d
using iter: 
64
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 64
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 9.413004e-03
Time for last run: 1.006508e-02
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 528384000
Total Bytes 0
Elapsed Time 1.006508e-02 seconds
FLOP/s 5.249676e+10
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:17:32 PST 2022
using iter: 
512
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 512
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 1.315308e-02
Time for last run: 1.359701e-02
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 4198400000
Total Bytes 0
Elapsed Time 1.359701e-02 seconds
FLOP/s 3.087737e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:17:34 PST 2022
using iter: 
4096
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 4096
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 4.148197e-02
Time for last run: 4.165506e-02
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 33558528000
Total Bytes 0
Elapsed Time 4.165506e-02 seconds
FLOP/s 8.056290e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:18:06 PST 2022
using iter: 
32768
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 32768
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 2.698300e-01
Time for last run: 2.763989e-01
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 268439552000
Total Bytes 0
Elapsed Time 2.763989e-01 seconds
FLOP/s 9.712034e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:18:10 PST 2022
using iter: 
262144
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 262144
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 2.132955e+00
Time for last run: 2.177703e+00
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 2147487744000
Total Bytes 0
Elapsed Time 2.177703e+00 seconds
FLOP/s 9.861252e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:18:20 PST 2022
using iter: 
2097152
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 2097152
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 1.706278e+01
Time for last run: 1.732607e+01
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 17179873280000
Total Bytes 0
Elapsed Time 1.732607e+01 seconds
FLOP/s 9.915621e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:18:58 PST 2022
using iter: 
16777216
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 16777216
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 1.373432e+02
Time for last run: 1.376995e+02
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 137438957568000
Total Bytes 0
Elapsed Time 1.376995e+02 seconds
FLOP/s 9.981076e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:24:05 PST 2022
using iter: 
64
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 64
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 9.635925e-03
Time for last run: 1.034117e-02
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 528384000
Total Bytes 0
Elapsed Time 1.034117e-02 seconds
FLOP/s 5.109520e+10
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:24:11 PST 2022
using iter: 
512
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 512
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 1.327395e-02
Time for last run: 1.387477e-02
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 4198400000
Total Bytes 0
Elapsed Time 1.387477e-02 seconds
FLOP/s 3.025924e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:24:15 PST 2022
using iter: 
4096
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 4096
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 4.156590e-02
Time for last run: 4.197097e-02
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 33558528000
Total Bytes 0
Elapsed Time 4.197097e-02 seconds
FLOP/s 7.995653e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:24:47 PST 2022
using iter: 
32768
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 32768
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 2.764809e-01
Time for last run: 2.694409e-01
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 268439552000
Total Bytes 0
Elapsed Time 2.694409e-01 seconds
FLOP/s 9.962836e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:24:55 PST 2022
using iter: 
262144
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 262144
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 2.120747e+00
Time for last run: 2.221549e+00
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 2147487744000
Total Bytes 0
Elapsed Time 2.221549e+00 seconds
FLOP/s 9.666623e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:25:31 PST 2022
using iter: 
2097152
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 2097152
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 1.714119e+01
Time for last run: 1.726011e+01
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 17179873280000
Total Bytes 0
Elapsed Time 1.726011e+01 seconds
FLOP/s 9.953513e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:26:38 PST 2022
using iter: 
16777216
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 16777216
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 1.371196e+02
Time for last run: 1.383975e+02
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 137438957568000
Total Bytes 0
Elapsed Time 1.383975e+02 seconds
FLOP/s 9.930743e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:31:45 PST 2022
using iter: 
64
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 64
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 1.004004e-02
Time for last run: 9.958029e-03
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 528384000
Total Bytes 0
Elapsed Time 9.958029e-03 seconds
FLOP/s 5.306110e+10
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:32:17 PST 2022
using iter: 
512
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 512
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 1.345110e-02
Time for last run: 1.363015e-02
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 4198400000
Total Bytes 0
Elapsed Time 1.363015e-02 seconds
FLOP/s 3.080230e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:32:50 PST 2022
using iter: 
4096
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 4096
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 4.246092e-02
Time for last run: 4.164910e-02
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 33558528000
Total Bytes 0
Elapsed Time 4.164910e-02 seconds
FLOP/s 8.057443e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:33:21 PST 2022
using iter: 
32768
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 32768
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 2.697201e-01
Time for last run: 2.694678e-01
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 268439552000
Total Bytes 0
Elapsed Time 2.694678e-01 seconds
FLOP/s 9.961840e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:33:26 PST 2022
using iter: 
262144
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 262144
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 2.167069e+00
Time for last run: 2.177875e+00
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 2147487744000
Total Bytes 0
Elapsed Time 2.177875e+00 seconds
FLOP/s 9.860474e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:34:02 PST 2022
using iter: 
2097152
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 2097152
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 1.726637e+01
Time for last run: 1.738030e+01
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 17179873280000
Total Bytes 0
Elapsed Time 1.738030e+01 seconds
FLOP/s 9.884683e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:34:40 PST 2022
using iter: 
16777216
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 16777216
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 1.373790e+02
Time for last run: 1.376422e+02
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 137438957568000
Total Bytes 0
Elapsed Time 1.376422e+02 seconds
FLOP/s 9.985234e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:39:47 PST 2022
using iter: 
64
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 64
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 9.571075e-03
Time for last run: 1.008987e-02
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 528384000
Total Bytes 0
Elapsed Time 1.008987e-02 seconds
FLOP/s 5.236775e+10
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:40:19 PST 2022
using iter: 
512
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 512
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 1.318407e-02
Time for last run: 1.359916e-02
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 4198400000
Total Bytes 0
Elapsed Time 1.359916e-02 seconds
FLOP/s 3.087250e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:40:24 PST 2022
using iter: 
4096
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 4096
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 4.161310e-02
Time for last run: 4.169893e-02
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 33558528000
Total Bytes 0
Elapsed Time 4.169893e-02 seconds
FLOP/s 8.047815e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:40:31 PST 2022
using iter: 
32768
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 32768
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 2.697439e-01
Time for last run: 2.694752e-01
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 268439552000
Total Bytes 0
Elapsed Time 2.694752e-01 seconds
FLOP/s 9.961567e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:41:04 PST 2022
using iter: 
262144
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.009 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 262144
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 2.119538e+00
Time for last run: 2.173652e+00
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 2147487744000
Total Bytes 0
Elapsed Time 2.173652e+00 seconds
FLOP/s 9.879629e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:41:13 PST 2022
using iter: 
2097152
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 2097152
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 1.708928e+01
Time for last run: 1.737090e+01
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 17179873280000
Total Bytes 0
Elapsed Time 1.737090e+01 seconds
FLOP/s 9.890032e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:41:50 PST 2022
using iter: 
16777216
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 16777216
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 1.373010e+02
Time for last run: 1.375822e+02
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 137438957568000
Total Bytes 0
Elapsed Time 1.375822e+02 seconds
FLOP/s 9.989590e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:46:29 PST 2022
using iter: 
64
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 64
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 9.641886e-03
Time for last run: 1.041698e-02
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 528384000
Total Bytes 0
Elapsed Time 1.041698e-02 seconds
FLOP/s 5.072332e+10
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:46:34 PST 2022
using iter: 
512
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 512
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 1.364183e-02
Time for last run: 1.381707e-02
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 4198400000
Total Bytes 0
Elapsed Time 1.381707e-02 seconds
FLOP/s 3.038560e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:46:39 PST 2022
using iter: 
4096
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 4096
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 4.162288e-02
Time for last run: 4.200101e-02
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 33558528000
Total Bytes 0
Elapsed Time 4.200101e-02 seconds
FLOP/s 7.989934e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:46:42 PST 2022
using iter: 
32768
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 32768
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 2.766860e-01
Time for last run: 2.713170e-01
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 268439552000
Total Bytes 0
Elapsed Time 2.713170e-01 seconds
FLOP/s 9.893945e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:46:46 PST 2022
using iter: 
262144
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 262144
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
Time for last run: 2.129162e+00
Time for last run: 2.176131e+00
Total Tasks 64000
Total Dependencies 189810
  Unable to estimate local/nonlocal dependencies
Total FLOPs 2147487744000
Total Bytes 0
Elapsed Time 2.176131e+00 seconds
FLOP/s 9.868375e+11
B/s 0.000000e+00
Transfer (estimated):
  Unable to estimate local/nonlocal transfer
[Partition 0][Node 0] End of program
done iter =====================================  
Thu Jan 13 18:46:53 PST 2022
using iter: 
2097152
Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 32 processes, 1 worker threads (PEs) + 1 comm threads per process, 32 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-129-g001844861
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 1 hosts (2 sockets x 16 cores x 2 PUs = 64-way SMP)
Charm++> cpu topology info is gathered in 0.001 seconds.
[33] Stack Traceback:
[34] Stack Traceback:
[35] Stack Traceback:
[36] Stack Traceback:
[37] Stack Traceback:
[38] Stack Traceback:
[39] Stack Traceback:
[40] Stack Traceback:
[41] Stack Traceback:
[42] Stack Traceback:
[43] Stack Traceback:
  [43:0] benchmark_smp 0x2020e395 
  [43:1] libpthread.so.0 0x2aaaaace6310 
[44] Stack Traceback:
  [44:0] benchmark_smp 0x2020e395 
  [44:1] libpthread.so.0 0x2aaaaace6310 
[45] Stack Traceback:
  [45:0] benchmark_smp 0x2020e395 
  [45:1] libpthread.so.0 0x2aaaaace6310 
[46] Stack Traceback:
  [46:0] benchmark_smp 0x2020e395 
  [46:1] libpthread.so.0 0x2aaaaace6310 
[47] Stack Traceback:
  [47:0] benchmark_smp 0x2020e395 
  [47:1] libpthread.so.0 0x2aaaaace6310 
[48] Stack Traceback:
  [48:0] benchmark_smp 0x2020e395 
  [48:1] libpthread.so.0 0x2aaaaace6310 
[49] Stack Traceback:
  [49:0] benchmark_smp 0x2020e395 
  [49:1] libpthread.so.0 0x2aaaaace6310 
[50] Stack Traceback:
  [50:0] benchmark_smp 0x2020e395 
  [50:1] libpthread.so.0 0x2aaaaace6310 
  [50:2] libmpich_gnu_82.so.3 0x2aaaac0e54a4 MPIDI_CH3I_Progress
[51] Stack Traceback:
  [51:0] benchmark_smp 0x2020e395 
  [51:1] libpthread.so.0 0x2aaaaace6310 
  [51:2] libpthread.so.0 0x2aaaaace2c2b __pthread_getspecific
[52] Stack Traceback:
  [52:0] benchmark_smp 0x2020e395 
  [52:1] libpthread.so.0 0x2aaaaace6310 
  [52:2] libmpich_gnu_82.so.3 0x2aaaac0e4fc1 MPIDI_CH3I_Progress
[53] Stack Traceback:
  [53:0] benchmark_smp 0x2020e395 
  [53:1] libpthread.so.0 0x2aaaaace6310 
  [53:2] libmpich_gnu_82.so.3 0x2aaaabfe1aae PMPI_Iprobe
[54] Stack Traceback:
  [54:0] benchmark_smp 0x2020e395 
  [54:1] libpthread.so.0 0x2aaaaace6310 
  [54:2] libmpich_gnu_82.so.3 0x2aaaabfe1a1e PMPI_Iprobe
[55] Stack Traceback:
  [55:0] benchmark_smp 0x2020e395 
  [55:1] libpthread.so.0 0x2aaaaace6310 
  [55:2] libmpich_gnu_82.so.3 0x2aaaac0e4fc1 MPIDI_CH3I_Progress
[56] Stack Traceback:
  [56:0] benchmark_smp 0x2020e395 
  [56:1] libpthread.so.0 0x2aaaaace6310 
  [56:2] libmpich_gnu_82.so.3 0x2aaaac0e4fd2 MPIDI_CH3I_Progress
[57] Stack Traceback:
  [57:0] benchmark_smp 0x2020e395 
  [57:1] libpthread.so.0 0x2aaaaace6310 
  [57:2] libmpich_gnu_82.so.3 0x2aaaac0e4fba MPIDI_CH3I_Progress
[58] Stack Traceback:
  [58:0] benchmark_smp 0x2020e395 
  [58:1] libpthread.so.0 0x2aaaaace6310 
  [58:2] libmpich_gnu_82.so.3 0x2aaaac0e4a46 MPIDI_CH3I_Progress
[59] Stack Traceback:
  [59:0] benchmark_smp 0x2020e395 
  [59:1] libpthread.so.0 0x2aaaaace6310 
  [59:2] libmpich_gnu_82.so.3 0x2aaaac0e4857 MPIDI_CH3I_Progress
[60] Stack Traceback:
  [60:0] benchmark_smp 0x2020e395 
  [60:1] libpthread.so.0 0x2aaaaace6310 
  [60:2] libmpich_gnu_82.so.3 0x2aaaac0e4fba MPIDI_CH3I_Progress
[61] Stack Traceback:
  [61:0] benchmark_smp 0x2020e395 
  [61:1] libpthread.so.0 0x2aaaaace6310 
  [61:2] benchmark_smp 0x2020d510 
[62] Stack Traceback:
  [62:0] benchmark_smp 0x2020e395 
  [62:1] libpthread.so.0 0x2aaaaace6310 
  [62:2] libmpich_gnu_82.so.3 0x2aaaac084fd1 MPIDI_CH3U_Recvq_FU
  [62:3] libmpich_gnu_82.so.3 0x2aaaac0d0e70 MPID_Iprobe
[63] Stack Traceback:
  [63:0] benchmark_smp 0x2020e395 
  [63:1] libpthread.so.0 0x2aaaaace6310 
  [63:2] libmpich_gnu_82.so.3 0x2aaaac0d0f20 MPID_Iprobe
  [33:0] benchmark_smp 0x2020e395 
  [33:1] libpthread.so.0 0x2aaaaace6310 
  [33:2] libmpich_gnu_82.so.3 0x2aaaac0e4fe1 MPIDI_CH3I_Progress
  [33:3] libmpich_gnu_82.so.3 0x2aaaac0d0f09 MPID_Iprobe
  [33:4] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [34:0] benchmark_smp 0x2020e395 
  [34:1] libpthread.so.0 0x2aaaaace6310 
  [34:2] libmpich_gnu_82.so.3 0x2aaaac0e4fba MPIDI_CH3I_Progress
  [34:3] libmpich_gnu_82.so.3 0x2aaaac0d0f09 MPID_Iprobe
  [34:4] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [35:0] benchmark_smp 0x2020e395 
  [35:1] libpthread.so.0 0x2aaaaace6310 
  [35:2] libmpich_gnu_82.so.3 0x2aaaac0e4fd2 MPIDI_CH3I_Progress
  [35:3] libmpich_gnu_82.so.3 0x2aaaac0d0f09 MPID_Iprobe
  [35:4] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [36:0] benchmark_smp 0x2020e395 
  [36:1] libpthread.so.0 0x2aaaaace6310 
  [36:2] libmpich_gnu_82.so.3 0x2aaaac0e4fb7 MPIDI_CH3I_Progress
  [36:3] libmpich_gnu_82.so.3 0x2aaaac0d0f09 MPID_Iprobe
  [36:4] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [37:0] benchmark_smp 0x2020e395 
  [37:1] libpthread.so.0 0x2aaaaace6310 
  [37:2] benchmark_smp 0x2020b7e4 
  [37:3] benchmark_smp 0x2020d51e 
  [38:0] benchmark_smp 0x2020e395 
  [38:1] libpthread.so.0 0x2aaaaace6310 
  [38:2] libmpich_gnu_82.so.3 0x2aaaac0e4fb0 MPIDI_CH3I_Progress
  [38:3] libmpich_gnu_82.so.3 0x2aaaac0d0f09 MPID_Iprobe
  [38:4] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [39:0] benchmark_smp 0x2020e395 
  [39:1] libpthread.so.0 0x2aaaaace6310 
  [39:2] libmpich_gnu_82.so.3 0x2aaaac0e4fdb MPIDI_CH3I_Progress
  [39:3] libmpich_gnu_82.so.3 0x2aaaac0d0f09 MPID_Iprobe
  [39:4] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [40:0] benchmark_smp 0x2020e395 
  [40:1] libpthread.so.0 0x2aaaaace6310 
  [40:2] libpthread.so.0 0x2aaaaace2c38 __pthread_getspecific
  [40:3] benchmark_smp 0x2020b807 
  [40:4] benchmark_smp 0x2020d51e 
  [41:0] benchmark_smp 0x2020e395 
  [41:1] libpthread.so.0 0x2aaaaace6310 
  [41:2] libpthread.so.0 0x2aaaaacdf645 
  [41:3] benchmark_smp 0x2020d94d 
  [41:4] benchmark_smp 0x2020dfce ConverseInit
  [42:0] benchmark_smp 0x2020e395 
  [42:1] libpthread.so.0 0x2aaaaace6310 
  [42:2] libmpich_gnu_82.so.3 0x2aaaac0e4fcf MPIDI_CH3I_Progress
  [42:3] libmpich_gnu_82.so.3 0x2aaaac0d0f09 MPID_Iprobe
  [42:4] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [43:2] libmpich_gnu_82.so.3 0x2aaaac0e4fd2 MPIDI_CH3I_Progress
  [43:3] libmpich_gnu_82.so.3 0x2aaaac0d0f09 MPID_Iprobe
  [43:4] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [44:2] libmpich_gnu_82.so.3 0x2aaaac0e4fcf MPIDI_CH3I_Progress
  [44:3] libmpich_gnu_82.so.3 0x2aaaac0d0f09 MPID_Iprobe
  [44:4] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [45:2] libmpich_gnu_82.so.3 0x2aaaabfe1a26 PMPI_Iprobe
  [45:3] benchmark_smp 0x2020c965 
  [45:4] benchmark_smp 0x2020d519 
  [46:2] libmpich_gnu_82.so.3 0x2aaaac0e4fd2 MPIDI_CH3I_Progress
  [46:3] libmpich_gnu_82.so.3 0x2aaaac0d0f09 MPID_Iprobe
  [46:4] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [46:5] benchmark_smp 0x2020c965 
  [47:2] libmpich_gnu_82.so.3 0x2aaaabfe1ae1 PMPI_Iprobe
  [47:3] benchmark_smp 0x2020c965 
  [47:4] benchmark_smp 0x2020d519 
  [48:2] benchmark_smp 0x2020c960 
  [48:3] benchmark_smp 0x2020d519 
  [48:4] benchmark_smp 0x2020d948 
  [49:2] libmpich_gnu_82.so.3 0x2aaaac0e4fcc MPIDI_CH3I_Progress
  [49:3] libmpich_gnu_82.so.3 0x2aaaac0d0f09 MPID_Iprobe
  [49:4] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [49:5] benchmark_smp 0x2020c965 
  [50:3] libmpich_gnu_82.so.3 0x2aaaac0d0f09 MPID_Iprobe
  [50:4] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [50:5] benchmark_smp 0x2020c965 
  [51:3] benchmark_smp 0x2020b807 
  [51:4] benchmark_smp 0x2020d51e 
  [52:3] libmpich_gnu_82.so.3 0x2aaaac0d0f09 MPID_Iprobe
  [52:4] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [52:5] benchmark_smp 0x2020c965 
  [53:3] benchmark_smp 0x2020c965 
  [53:4] benchmark_smp 0x2020d519 
  [54:3] benchmark_smp 0x2020c965 
  [54:4] benchmark_smp 0x2020d519 
  [55:3] libmpich_gnu_82.so.3 0x2aaaac0d0f09 MPID_Iprobe
  [55:4] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [55:5] benchmark_smp 0x2020c965 
  [56:3] libmpich_gnu_82.so.3 0x2aaaac0d0f09 MPID_Iprobe
  [56:4] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [56:5] benchmark_smp 0x2020c965 
  [57:3] libmpich_gnu_82.so.3 0x2aaaac0d0f09 MPID_Iprobe
  [57:4] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [57:5] benchmark_smp 0x2020c965 
  [58:3] libmpich_gnu_82.so.3 0x2aaaac0d0f09 MPID_Iprobe
  [58:4] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [58:5] benchmark_smp 0x2020c965 
  [59:3] libmpich_gnu_82.so.3 0x2aaaac0d0f09 MPID_Iprobe
  [59:4] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [59:5] benchmark_smp 0x2020c965 
  [60:3] libmpich_gnu_82.so.3 0x2aaaac0d0f09 MPID_Iprobe
  [60:4] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [60:5] benchmark_smp 0x2020c965 
  [61:3] benchmark_smp 0x2020d948 
  [61:4] benchmark_smp 0x2020dfce ConverseInit
  [62:4] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [62:5] benchmark_smp 0x2020c965 
  [63:3] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [63:4] benchmark_smp 0x2020c965 
  [33:5] benchmark_smp 0x2020c965 
  [34:5] benchmark_smp 0x2020c965 
  [34:6] benchmark_smp 0x2020d519 
  [35:5] benchmark_smp 0x2020c965 
  [36:5] benchmark_smp 0x2020c965 
  [36:6] benchmark_smp 0x2020d519 
  [37:4] benchmark_smp 0x2020d93e 
  [38:5] benchmark_smp 0x2020c965 
  [39:5] benchmark_smp 0x2020c965 
  [39:6] benchmark_smp 0x2020d519 
  [40:5] benchmark_smp 0x2020d93e 
  [41:5] benchmark_smp 0x201649f7 charm_main
  [41:6] libc.so.6 0x2aaaaca5234a __libc_start_main
  [42:5] benchmark_smp 0x2020c965 
  [42:6] benchmark_smp 0x2020d519 
  [43:5] benchmark_smp 0x2020c965 
  [43:6] benchmark_smp 0x2020d519 
  [44:5] benchmark_smp 0x2020c965 
  [44:6] benchmark_smp 0x2020d519 
  [45:5] benchmark_smp 0x2020d948 
  [46:6] benchmark_smp 0x2020d519 
  [47:5] benchmark_smp 0x2020d93e 
  [48:5] benchmark_smp 0x2020dfce ConverseInit
  [50:6] benchmark_smp 0x2020d519 
  [51:5] benchmark_smp 0x2020d948 
  [51:6] benchmark_smp 0x2020dfce ConverseInit
  [52:6] benchmark_smp 0x2020d519 
  [54:5] benchmark_smp 0x2020d93e 
  [54:6] benchmark_smp 0x2020dfce ConverseInit
  [33:6] benchmark_smp 0x2020d519 
  [35:6] benchmark_smp 0x2020d519 
  [36:7] benchmark_smp 0x2020d948 
  [37:5] benchmark_smp 0x2020dfce ConverseInit
  [38:6] benchmark_smp 0x2020d519 
  [40:6] benchmark_smp 0x2020dfce ConverseInit
  [41:7] benchmark_smp 0x200d882a _start
  [44:7] benchmark_smp 0x2020d948 
  [45:6] benchmark_smp 0x2020dfce ConverseInit
  [46:7] benchmark_smp 0x2020d948 
  [47:6] benchmark_smp 0x2020dfce ConverseInit
  [49:6] benchmark_smp 0x2020d519 
  [49:7] benchmark_smp 0x2020d93e 
  [50:7] benchmark_smp 0x2020d93e 
  [52:7] benchmark_smp 0x2020d93e 
  [53:5] benchmark_smp 0x2020d948 
  [53:6] benchmark_smp 0x2020dfce ConverseInit
  [55:6] benchmark_smp 0x2020d519 
  [55:7] benchmark_smp 0x2020d948 
  [56:6] benchmark_smp 0x2020d519 
  [56:7] benchmark_smp 0x2020d93e 
  [57:6] benchmark_smp 0x2020d519 
  [57:7] benchmark_smp 0x2020d948 
  [58:6] benchmark_smp 0x2020d519 
  [58:7] benchmark_smp 0x2020d948 
  [59:6] benchmark_smp 0x2020d519 
  [59:7] benchmark_smp 0x2020d93e 
  [60:6] benchmark_smp 0x2020d519 
  [60:7] benchmark_smp 0x2020d93e 
  [61:5] benchmark_smp 0x201649f7 charm_main
  [61:6] libc.so.6 0x2aaaaca5234a __libc_start_main
  [61:7] benchmark_smp 0x200d882a _start
  [62:6] benchmark_smp 0x2020d519 
  [62:7] benchmark_smp 0x2020d93e 
  [62:8] benchmark_smp 0x2020dfce ConverseInit
  [63:5] benchmark_smp 0x2020d519 
  [63:6] benchmark_smp 0x2020d948 
  [33:7] benchmark_smp 0x2020d948 
  [33:8] benchmark_smp 0x2020dfce ConverseInit
  [34:7] benchmark_smp 0x2020d93e 
  [34:8] benchmark_smp 0x2020dfce ConverseInit
  [35:7] benchmark_smp 0x2020d948 
  [35:8] benchmark_smp 0x2020dfce ConverseInit
  [36:8] benchmark_smp 0x2020dfce ConverseInit
  [37:6] benchmark_smp 0x201649f7 charm_main
  [37:7] libc.so.6 0x2aaaaca5234a __libc_start_main
  [37:8] benchmark_smp 0x200d882a _start
  [38:7] benchmark_smp 0x2020d93e 
  [38:8] benchmark_smp 0x2020dfce ConverseInit
  [39:7] benchmark_smp 0x2020d948 
  [39:8] benchmark_smp 0x2020dfce ConverseInit
  [40:7] benchmark_smp 0x201649f7 charm_main
  [40:8] libc.so.6 0x2aaaaca5234a __libc_start_main
  [40:9] benchmark_smp 0x200d882a _start
  [42:7] benchmark_smp 0x2020d93e 
  [42:8] benchmark_smp 0x2020dfce ConverseInit
  [43:7] benchmark_smp 0x2020d93e 
  [43:8] benchmark_smp 0x2020dfce ConverseInit
  [43:9] benchmark_smp 0x201649f7 charm_main
  [44:8] benchmark_smp 0x2020dfce ConverseInit
  [45:7] benchmark_smp 0x201649f7 charm_main
  [45:8] libc.so.6 0x2aaaaca5234a __libc_start_main
  [45:9] benchmark_smp 0x200d882a _start
  [46:8] benchmark_smp 0x2020dfce ConverseInit
  [46:9] benchmark_smp 0x201649f7 charm_main
  [46:10] libc.so.6 0x2aaaaca5234a __libc_start_main
  [47:7] benchmark_smp 0x201649f7 charm_main
  [47:8] libc.so.6 0x2aaaaca5234a __libc_start_main
  [47:9] benchmark_smp 0x200d882a _start
  [48:6] benchmark_smp 0x201649f7 charm_main
  [48:7] libc.so.6 0x2aaaaca5234a __libc_start_main
  [48:8] benchmark_smp 0x200d882a _start
  [49:8] benchmark_smp 0x2020dfce ConverseInit
  [49:9] benchmark_smp 0x201649f7 charm_main
  [50:8] benchmark_smp 0x2020dfce ConverseInit
  [50:9] benchmark_smp 0x201649f7 charm_main
  [50:10] libc.so.6 0x2aaaaca5234a __libc_start_main
  [51:7] benchmark_smp 0x201649f7 charm_main
  [51:8] libc.so.6 0x2aaaaca5234a __libc_start_main
  [51:9] benchmark_smp 0x200d882a _start
  [52:8] benchmark_smp 0x2020dfce ConverseInit
  [52:9] benchmark_smp 0x201649f7 charm_main
  [52:10] libc.so.6 0x2aaaaca5234a __libc_start_main
  [53:7] benchmark_smp 0x201649f7 charm_main
  [53:8] libc.so.6 0x2aaaaca5234a __libc_start_main
  [53:9] benchmark_smp 0x200d882a _start
  [54:7] benchmark_smp 0x201649f7 charm_main
  [54:8] libc.so.6 0x2aaaaca5234a __libc_start_main
  [54:9] benchmark_smp 0x200d882a _start
  [55:8] benchmark_smp 0x2020dfce ConverseInit
  [55:9] benchmark_smp 0x201649f7 charm_main
  [55:10] libc.so.6 0x2aaaaca5234a __libc_start_main
  [56:8] benchmark_smp 0x2020dfce ConverseInit
  [56:9] benchmark_smp 0x201649f7 charm_main
  [56:10] libc.so.6 0x2aaaaca5234a __libc_start_main
  [58:8] benchmark_smp 0x2020dfce ConverseInit
  [58:9] benchmark_smp 0x201649f7 charm_main
  [58:10] libc.so.6 0x2aaaaca5234a __libc_start_main
  [59:8] benchmark_smp 0x2020dfce ConverseInit
  [59:9] benchmark_smp 0x201649f7 charm_main
  [59:10] libc.so.6 0x2aaaaca5234a __libc_start_main
  [60:8] benchmark_smp 0x2020dfce ConverseInit
  [60:9] benchmark_smp 0x201649f7 charm_main
  [60:10] libc.so.6 0x2aaaaca5234a __libc_start_main
  [33:9] benchmark_smp 0x201649f7 charm_main
  [33:10] libc.so.6 0x2aaaaca5234a __libc_start_main
  [34:9] benchmark_smp 0x201649f7 charm_main
  [34:10] libc.so.6 0x2aaaaca5234a __libc_start_main
  [34:11] benchmark_smp 0x200d882a _start
  [35:9] benchmark_smp 0x201649f7 charm_main
  [35:10] libc.so.6 0x2aaaaca5234a __libc_start_main
  [36:9] benchmark_smp 0x201649f7 charm_main
  [36:10] libc.so.6 0x2aaaaca5234a __libc_start_main
  [36:11] benchmark_smp 0x200d882a _start
  [38:9] benchmark_smp 0x201649f7 charm_main
  [38:10] libc.so.6 0x2aaaaca5234a __libc_start_main
  [39:9] benchmark_smp 0x201649f7 charm_main
  [39:10] libc.so.6 0x2aaaaca5234a __libc_start_main
  [42:9] benchmark_smp 0x201649f7 charm_main
  [42:10] libc.so.6 0x2aaaaca5234a __libc_start_main
  [42:11] benchmark_smp 0x200d882a _start
  [43:10] libc.so.6 0x2aaaaca5234a __libc_start_main
  [43:11] benchmark_smp 0x200d882a _start
  [44:9] benchmark_smp 0x201649f7 charm_main
  [44:10] libc.so.6 0x2aaaaca5234a __libc_start_main
  [44:11] benchmark_smp 0x200d882a _start
  [46:11] benchmark_smp 0x200d882a _start
  [49:10] libc.so.6 0x2aaaaca5234a __libc_start_main
  [49:11] benchmark_smp 0x200d882a _start
  [50:11] benchmark_smp 0x200d882a _start
  [52:11] benchmark_smp 0x200d882a _start
  [57:8] benchmark_smp 0x2020dfce ConverseInit
  [57:9] benchmark_smp 0x201649f7 charm_main
  [57:10] libc.so.6 0x2aaaaca5234a __libc_start_main
  [57:11] benchmark_smp 0x200d882a _start
  [62:9] benchmark_smp 0x201649f7 charm_main
  [62:10] libc.so.6 0x2aaaaca5234a __libc_start_main
  [62:11] benchmark_smp 0x200d882a _start
  [63:7] benchmark_smp 0x2020dfce ConverseInit
  [63:8] benchmark_smp 0x201649f7 charm_main
  [63:9] libc.so.6 0x2aaaaca5234a __libc_start_main
  [33:11] benchmark_smp 0x200d882a _start
  [35:11] benchmark_smp 0x200d882a _start
  [38:11] benchmark_smp 0x200d882a _start
  [39:11] benchmark_smp 0x200d882a _start
  [55:11] benchmark_smp 0x200d882a _start
  [56:11] benchmark_smp 0x200d882a _start
  [58:11] benchmark_smp 0x200d882a _start
  [59:11] benchmark_smp 0x200d882a _start
  [60:11] benchmark_smp 0x200d882a _start
  [63:10] benchmark_smp 0x200d882a _start
Running Task Benchmark
  Configuration:
    Task Graph 1:
      Time Steps: 1000
      Max Width: 64
      Dependence Type: stencil_1d
      Radix: 3
      Period: 0
      Fraction Connected: 0.250000
      Kernel:
        Type: compute_bound
        Iterations: 2097152
        Samples: 16
        Imbalance: 0.000000
      Output Bytes: 16
      Scratch Bytes: 0
[32] Stack Traceback:
  [32:0] benchmark_smp 0x2020e395 
  [32:1] libpthread.so.0 0x2aaaaace6310 
  [32:2] libmpich_gnu_82.so.3 0x2aaaac0e4fb0 MPIDI_CH3I_Progress
  [32:3] libmpich_gnu_82.so.3 0x2aaaac0d0f09 MPID_Iprobe
  [32:4] libmpich_gnu_82.so.3 0x2aaaabfe1adc PMPI_Iprobe
  [32:5] benchmark_smp 0x2020c965 
  [32:6] benchmark_smp 0x2020d519 
  [32:7] benchmark_smp 0x2020d93e 
  [32:8] benchmark_smp 0x2020dfce ConverseInit
  [32:9] benchmark_smp 0x201649f7 charm_main
  [32:10] libc.so.6 0x2aaaaca5234a __libc_start_main
  [32:11] benchmark_smp 0x200d882a _start
